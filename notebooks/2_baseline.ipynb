{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0d18063",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dab66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root = \"../data\"\n",
    "batch_size = 16\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
    "\n",
    "test = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366841ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.shape, train.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7491e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f7f4f",
   "metadata": {},
   "source": [
    "##### Split training data into 50k training instances and 10k validation instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c60176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train, \n",
    "    [train_size, val_size], \n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8292422",
   "metadata": {},
   "source": [
    "##### Training loop (we will finish it on question 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ccaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfc421",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93369534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad7267",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002d927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1, stride=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e6dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf958e7d",
   "metadata": {},
   "source": [
    "### Finished Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cde9f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.358\n",
      "[2,  2000] loss: 0.086\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495c447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f989dd",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b47150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = './mnist_net.pth'\n",
    "#torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c459b6",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c22bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 validation images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 validation images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0f354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19d47d",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b72328",
   "metadata": {},
   "source": [
    "### Applying transformations to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e602036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(78.5675)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset.data.float().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e297e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "mnist_mean = train_dataset.dataset.data.float().mean().item()\n",
    "mnist_std = train_dataset.dataset.data.float().std().item()\n",
    "\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomCrop(size=(28, 28), padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0], std=[0.2]),\n",
    "])\n",
    "\n",
    "transform_base = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "base_train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform_base)\n",
    "\n",
    "augmented_train = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform_aug)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb586c8",
   "metadata": {},
   "source": [
    "### Rerun the training loop with base (Now with the complete training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "814d07e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.353\n",
      "[2,  2000] loss: 0.079\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "base_train_loader = torch.utils.data.DataLoader(base_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Initialize the model\n",
    "net = Net()\n",
    "\n",
    "# Initialize criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0003)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(base_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e802e4b",
   "metadata": {},
   "source": [
    "### Test the baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27061c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2d324",
   "metadata": {},
   "source": [
    "### Run training loop with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ee2486cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.382\n",
      "[2,  2000] loss: 0.096\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "augmented_train_loader = torch.utils.data.DataLoader(augmented_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Initialize the model\n",
    "net = Net()\n",
    "\n",
    "# Initialize criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0003)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(augmented_train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11f15b",
   "metadata": {},
   "source": [
    "### Test the augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "adfc5c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f90fe",
   "metadata": {},
   "source": [
    "### Why do we only augment the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc268750",
   "metadata": {},
   "source": [
    "It is considered best practice to avoid applying data augmentation to the validation and test sets, as this can compromise the reliability of model evaluation. The purpose of augmentation is to artificially increase the diversity of the training data in order to improve the model’s ability to generalize. However, augmenting the validation or test data can significantly distort evaluation metrics. \n",
    "For example, flipping an image of the digit 9 in the test set might transform it into something that resembles a 6. If the model then predicts 6, this would be recorded as an incorrect classification, even though the prediction would be reasonable given the augmented input. Such transformations introduce inconsistencies that make the evaluation no longer reflect performance on the true underlying data distribution. Therefore, augmenting validation or test data can harm the evaluation process, and augmentation should only be applied to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d6e78",
   "metadata": {},
   "source": [
    "# Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e99df8",
   "metadata": {},
   "source": [
    "### Let's load the ImageFolder data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bbdba2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms \n",
    "\n",
    "root = \"../data/mnist-varres\"\n",
    "batch_size = 16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(root=root + \"/train\", transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test = torchvision.datasets.ImageFolder(root=root + \"/test\", transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5989d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ffb40da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a28a46",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cfa2b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.070\n",
      "[1,   400] loss: 0.035\n",
      "[1,   600] loss: 0.019\n",
      "[2,   200] loss: 0.012\n",
      "[2,   400] loss: 0.008\n",
      "[2,   600] loss: 0.007\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "net = Net()\n",
    "\n",
    "# Initialize criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0003)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f091a",
   "metadata": {},
   "source": [
    "### Now, we test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ce647",
   "metadata": {},
   "source": [
    "##### As expected, reducing the image size had a strong negative impact on accuracy. This helped us verify that when the images are downsampled, a significant amount of visual information is lost. \n",
    "##### Since the classifier received less detailed representations of each image, its ability to correctly identify digits decreased, leading to a lower performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7a5e1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 20 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1016eb",
   "metadata": {},
   "source": [
    "# Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa5720",
   "metadata": {},
   "source": [
    "As shown in the 64×64 version of an image from the MNIST dataset (Figure \\ref{fig:ex13_zero_64}), resizing the images to 64×64 simply adds extra black pixels that contain no useful information. This additional padding may cause the model to focus on irrelevant regions of the image, which can negatively impact its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
